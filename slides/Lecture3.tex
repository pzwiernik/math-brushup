\documentclass[11pt,aspectratio=169]{beamer}
\input{./defs.tex}




% Local macros for this lecture
\DeclareMathOperator{\rank}{rank}

\title[Calculus and Linear Algebra]{Lecture 3: Calculus and Linear Algebra}
\author[Piotr Zwiernik, Barcelona School of Economics]{Piotr Zwiernik \\[4pt]
Mathematics Brush-up\\[8pt]
\includegraphics[width=1.2in]{img/bse.png}}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

%==============================
%   CHAPTER 7: LINEAR SYSTEMS
%==============================
\begin{frame}{Chapter 7: Systems of Linear Equations}
 
Many problems in economics and data science can be modeled as a \textcolor{SeaGreen}{system of linear equations}.
\vskip 8pt
In this chapter we recall core facts and \textcolor{SeaGreen}{general solution procedures}.
\vskip 8pt
\textcolor{SeaGreen}{Read} Chapter 8 of Werner--Sotskov; Chapter 7 of Simon--Blume. \quad
\bigskip

\textcolor{SeaGreen}{Exercises} 8.5, 8.6, 8.8 (Werner--Sotskov).
 
\end{frame}

\begin{frame}{Systems of linear equations}
 
Let $A$ be an $m\times n$ matrix, ${\bf x}=(x_1,\ldots,x_n)\in\R^n$ unknown, and ${\bf b}\in\R^m$ given. The system
\[
A{\bf x}={\bf b}
\]
has the \textcolor{SeaGreen}{vector form}
\[
\sum_{j=1}^n x_j\,{\bf a}_j={\bf b},
\]
where ${\bf a}_1,\dots,{\bf a}_n$ are the columns of $A$.\\[6mm]

If ${\bf b}={\bf 0}$ the system is \textcolor{SeaGreen}{homogeneous}; otherwise it is \textcolor{SeaGreen}{inhomogeneous}.\\[3mm] 
An inhomogeneous system can be \textcolor{SeaGreen}{inconsistent} (no solution) or \textcolor{SeaGreen}{consistent} (at least one solution).\\[3mm]
 A homogeneous system always has ${\bf x}={\bf 0}$ as a solution.
 \end{frame}

% --- Motivating example: modern & concrete (PageRank) ---
\begin{frame}{Motivating example: ranking the web (PageRank)}

Modern search engines rank pages by solving a linear system. Build a row-stochastic transition matrix \(P\) where \(P_{ij}\) is the probability to move from page \(i\) to page \(j\). The long-run visit probabilities \(\mathbf{x}\) solve
\[
\big(I-\alpha P^{\top}\big)\,\mathbf{x}=(1-\alpha)\,\mathbf{v},\quad 0<\alpha<1,
\]
with \(\mathbf{v}\) a teleport vector (often uniform).

\textcolor{SeaGreen}{Mini-web example}: pages \(A,B,C,D\) with links
\(A\to\{B,C\}\), \(B\to C\), \(C\to A\), \(D\to\{A,C\}\). Then
\[
{\scriptsize P=\begin{pmatrix}
0 & \tfrac12 & \tfrac12 & 0\\
0 & 0 & 1 & 0\\
1 & 0 & 0 & 0\\
\tfrac12 & 0 & \tfrac12 & 0
\end{pmatrix}},\quad
\alpha=0.85,\ \mathbf{v}=\tfrac14(1,1,1,1)^{\top}.
\]
Solving gives
\[
\mathbf{x}\approx (0.380,0.199,0.384,0.038),
\]
so \(C\) and \(A\) rank highest.\\[2mm] Node \(D\) is rarely visited because nobody links to it (only teleportation contributes).
\end{frame}

% --- Alternative motivation that connects to least squares later ---
\begin{frame}{Motivating example: deblurring a photo}
\begin{small}
Phone photos often suffer from motion blur. Blurring is (approximately) linear:
\[
b=A\,x+\text{noise},
\]
where \(x\) is the unknown sharp image (vectorized), \(b\) is the observed blurred image, and \(A\) encodes the blur kernel (convolution).

\begin{itemize}
\item \textcolor{SeaGreen}{Ideal case}: noise-free, known \(A\) \(\Rightarrow\) solve the linear system \(A x=b\).
\item \textcolor{SeaGreen}{Realistic case}: noisy or overdetermined \(\Rightarrow\) solve a least-squares problem
\[
\min_{x}\ \|A x-b\|^2+\lambda\|x\|^2,
\]
which leads to the normal equations \((A^{\top}A+\lambda  I_n)x=A^{\top}b\).
\end{itemize}

Same linear algebra ideas: vector form, consistency, rank, and algorithms to solve large systems.
\end{small}
\end{frame}


\begin{frame}{Rank and existence/uniqueness}
 \begin{alertblock}{Theorem (Column rank = Row rank)}
 	The maximum number of linearly independent columns of $A$ equals the maximum number of linearly independent rows. This common number is the \textcolor{SeaGreen}{rank} of $A$, denoted $\rank(A)$.
 \end{alertblock}

\vskip 6pt
\textcolor{SeaGreen}{Fact:} $\rank(A)$ equals the largest order of a \textcolor{SeaGreen}{minor} (determinant of a square submatrix) that is nonzero.
\vskip 8pt
\textcolor{SeaGreen}{Example:}
\[
A=\begin{pmatrix}
1&2&0\\
4&6&2\\
3&2&4
\end{pmatrix}
\]
Here $\det(A)=0$, so $\rank(A)\le 2$. Since $\det(A_{33})=\det\!\begin{pmatrix}1&2\\4&6\end{pmatrix}=-2\ne 0$, we get $\rank(A)=2$.
\vskip 4pt
{\tiny Note $2{\bf a}_1-{\bf a}_2={\bf a}_3$, but no single column is a multiple of another. Any two of the three columns form a basis of the column space.}
 
\end{frame}

\begin{frame}{Computing the rank with Gaussian elimination}
 
Elementary row or column operations do not change $\rank(A)$.
\vskip 6pt
\[
\begin{aligned}
\begin{pmatrix}
1&2&0\\
4&6&2\\
3&2&4
\end{pmatrix}
&\;\sim\;
\begin{pmatrix}
1&2&0\\
0&-2&2\\
0&-4&4
\end{pmatrix}
\;\sim\;
\begin{pmatrix}
1&2&0\\
0&-2&2\\
0&0&0
\end{pmatrix}
\end{aligned}
\]
The \textcolor{blue}{pivot} in each nonzero row is the first nonzero entry from the left.
\vskip 4pt
There are $2$ pivots, hence $\rank(A)=2$.
\vskip 6pt
To find a linear relation among columns, solve the homogeneous system for $(\lambda_1,\lambda_2,\lambda_3)$:
\[
\lambda_1+2\lambda_2=0,\qquad -\lambda_2+\lambda_3=0.
\]
Thus $(\lambda_1,\lambda_2,\lambda_3)=(-2\lambda_3,\lambda_3,\lambda_3)$ and, e.g., for $\lambda_3=1$,
\[
-2{\bf a}_1+{\bf a}_2+{\bf a}_3={\bf 0}.
\]
 
\end{frame}

\begin{frame}{Augmented matrix; consistency}
 
The \textcolor{SeaGreen}{augmented matrix} of the linear system $A\bs x=\bs b$ is $A_{\bf b}=\big[A\ \ {\bf b}\big]\in\R^{m\times(n+1)}$.\\[4mm]
\textcolor{SeaGreen}{Remark:} Either $\rank(A_{\bf b})=\rank(A)$ or $\rank(A_{\bf b})=\rank(A)+1$.\\[4mm]
\textcolor{SeaGreen}{Theorem (RouchÃ©--Capelli):} $A{\bf x}={\bf b}$ is \textcolor{SeaGreen}{consistent} $\Longleftrightarrow$ $\rank(A_{\bf b})=\rank(A)$ \ {\tiny (equivalently ${\bf b}\in\mathrm{span}\{{\bf a}_1,\ldots,{\bf a}_n\}$).}\\[6mm]

\textcolor{SeaGreen}{Moreover:}
\begin{enumerate}
\item If $\rank(A)=\rank(A_{\bf b})=n$, the solution is \textcolor{SeaGreen}{unique}. \alert{Q:} What is it?
\item If $\rank(A)=\rank(A_{\bf b})<n$, there are \textcolor{SeaGreen}{infinitely many} solutions; choose $n-\rank(A)$ variables \textcolor{SeaGreen}{free}.
\end{enumerate}
 
\end{frame}

\begin{frame}{Triangularization (Gaussian elimination)}
 \begin{alertblock}{Theorem}
Applying elementary row operations to $A_{\bf b}$ transforms any system into an equivalent \textcolor{SeaGreen}{upper-triangular} one. 	
 \end{alertblock}
 {\tiny This follows essentially from the fact that $A\bs x=\bs b$ $\Leftrightarrow$ $A_b \begin{bmatrix}
 	\x\\
 	-1
 \end{bmatrix}=\bs 0$ $\Leftrightarrow$ $CA_b \begin{bmatrix}
 	\x\\
 	-1
 \end{bmatrix}=\bs 0$ for any invertible $C$}
\[
\begin{aligned}
x_1+x_2+x_3&=3\\
x_1-x_2+2x_3&=2\\
4x_1+6x_2-x_3&=9
\end{aligned}
\qquad\Longleftrightarrow\qquad
\begin{aligned}
x_1+x_2+x_3&=3\\
-2x_2+x_3&=-1\\
-4x_3&=-4
\end{aligned}
\]
{\tiny Row ops: $r_2\!\leftarrow r_2-r_1$, $r_3\!\leftarrow r_3-4r_1$, $r_3\!\leftarrow r_3+r_2$.}
\vskip 4pt
We have $\rank(A)=\rank(A_{\bf b})=3$ $\Rightarrow$ unique solution ${\bf x}=(1,1,1)$.
 
\end{frame}

\begin{frame}{A homogeneous example}
 
Consider
\[
\begin{aligned}
a+b+c+d+e+f&=0,\\
2a+2b+2c+2d-e-f&=0,\\
3a+3b-c-d-e-f&=0.
\end{aligned}
\]
Row-reducing the coefficient matrix gives
\[
\begin{pmatrix}
1&1&0&0&0&0\\
0&0&1&1&0&0\\
0&0&0&0&1&1
\end{pmatrix},
\]
so there are $3$ pivots, $\rank=3$, and the solution space has dimension $6-\rank=3$:
\[
(a,-a,c,-c,e,-e).
\]
{\tiny A basis is $\{(1,-1,0,0,0,0),(0,0,1,-1,0,0),(0,0,0,0,1,-1)\}$.}
 
\end{frame}

\begin{frame}{General solution structure}
 
\textcolor{SeaGreen}{Theorem:} Every solution of $A{\bf x}={\bf b}$ can be written as
\[
{\bf x}={\bf x}_h+{\bf x}_p,
\]
where ${\bf x}_h$ is the \textcolor{SeaGreen}{general solution} of $A{\bf x}={\bf 0}$ and ${\bf x}_p$ is any \textcolor{SeaGreen}{particular solution} of $A{\bf x}={\bf b}$.
\vskip 6pt
\textcolor{SeaGreen}{Corollary:} If $A{\bf x}={\bf b}$ is consistent, it has as many solutions as $A{\bf x}={\bf 0}$.
\vskip 6pt
\textcolor{SeaGreen}{Example:} Three identical equations
\[
x_1+x_2+x_3=1
\]
yield $\rank(A)=\rank(A_{\bf b})=1$. Choose two free variables:
\[
\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}
=\lambda_1\!\begin{pmatrix}-1\\1\\0\end{pmatrix}
+\lambda_2\!\begin{pmatrix}-1\\0\\1\end{pmatrix}
+\begin{pmatrix}1\\0\\0\end{pmatrix},\quad \lambda_1,\lambda_2\in\R.
\]
\end{frame}


\begin{frame}{Two quick examples}
 
\begin{enumerate}
\item For
\(
A=\begin{pmatrix}
1&2&0\\
4&6&2\\
3&2&4
\end{pmatrix}
\)
we found $\rank(A)=2$. Thus $\dim\ker(A)=3-2=1$ and
\[
\ker(A)=\{\lambda(-2,1,1):\ \lambda\in\R\}.
\]
\item Consider
\[
\begin{aligned}
x_1+x_2+x_3&=-1,\\
x_1+2x_2+4x_3&=2.
\end{aligned}
\]
Row reduction gives
\(
x_1+x_2+x_3=-1,\ \ x_2+3x_3=3
\),
so $\rank(A)=\rank(A_{\bf b})=2$. One free variable:
\[
\{\,\lambda(2,-3,1)+(-4,3,0):\ \lambda\in\R\,\}.
\]
\end{enumerate}
 
\end{frame}

\begin{frame}{Finding a basis for a span}
 
Let
\[
{\bf v}_1=\begin{pmatrix}2\\-1\\0\end{pmatrix},\quad
{\bf v}_2=\begin{pmatrix}3\\0\\-1\end{pmatrix},\quad
{\bf v}_3=\begin{pmatrix}0\\3\\-2\end{pmatrix}.
\]
Form the $3\times 3$ matrix with these as \textcolor{SeaGreen}{columns}:
\[
\begin{pmatrix}
2&-1&0\\
3&0&-1\\
0&3&-2
\end{pmatrix}\ \sim\
\begin{pmatrix}
1&0&-1/3\\
0&1&-2/3\\
0&0&0
\end{pmatrix}.
\]
Thus $\rank=2$ and $3{\bf v}_1-2{\bf v}_2+{\bf v}_3={\bf 0}$. A convenient basis of $\mathrm{span}\{{\bf v}_1,{\bf v}_2,{\bf v}_3\}$ is
\[
\left\{\begin{pmatrix}1\\0\\-1/3\end{pmatrix},\ \begin{pmatrix}0\\1\\-2/3\end{pmatrix}\right\}
\]
(or, equivalently, two of the original vectors that are independent).
 
\end{frame}

\begin{frame}{Standing on the shoulders of giants}
\begin{minipage}{5.5cm}
	\includegraphics[scale=.148]{vbreaks/tower}
\end{minipage}\begin{minipage}{10cm}
	``If I have seen further, it is by standing on the shoulders of giants.''  \textbf{Isaac Newton (1675)}.\\[4mm]
	Some of the giants of linear/matrix algebra:
	\begin{itemize}
		\item Gauss (Gaussian elimination, least squares, determinants)
		\item Cauchy (CauchyâSchwarz, eigenvalues, determinants)
		\item Laplace (Laplace expansion of determinants)
		\item Cayley (CayleyâHamilton theorem, matrix theory)
		\item Sylvester (matrix rank, invariant theory)
		\item Jordan (Jordan canonical form)
		\item Frobenius (matrix factorizations, linear algebra foundations)
	\end{itemize}
	(pictured here as the base of a \textit{castell})
\end{minipage}
\end{frame}

%========================================
%   CHAPTER 8: EIGENVALUES & QUADRATICS
%========================================
\begin{frame}{Chapter 8: Eigenvalues and Quadratic Forms}
 
\textcolor{SeaGreen}{Eigenvalues/eigenvectors} appear in growth models, Markov chains, PCA, and stability of dynamical systems.\\[4mm]
\textcolor{SeaGreen}{Quadratic forms} drive optimization via second-order conditions.\\[4mm]

\textcolor{SeaGreen}{Read} Chapter 10 of Werner--Sotskov (optional: Simon--Blume, Ch.~7). \\[4mm]
\textcolor{SeaGreen}{Exercises} 10.2; 10.5 A,B,C; 10.6(a,b). 
\end{frame}




\begin{frame}{Eigenvalues and eigenvectors: migration example}

Consider a population split between two regions: city ($x_t^C$) and countryside ($x_t^R$) at time $t$.
Suppose each year a fraction of people move between the two regions:
\[
\begin{pmatrix}x^C_{t+1}\\ x^R_{t+1}\end{pmatrix}
=
\begin{pmatrix}0.8 & 0.4 \\ 0.2 & 0.6\end{pmatrix}
\begin{pmatrix}x^C_t\\ x^R_t\end{pmatrix}
= A\begin{pmatrix}x^C_t\\ x^R_t\end{pmatrix}.
\]

\medskip
Here, $0.8$ means that $80\%$ of city dwellers stay in the city, while $0.4$
means that $40\%$ of rural dwellers move to the city in the next period.

\medskip
If the \textcolor{SeaGreen}{urban/rural ratio stabilizes}, the next state must be a constant multiple of the current state:
\[
\begin{pmatrix}x^C_{t+1}\\ x^R_{t+1}\end{pmatrix}
=\lambda \begin{pmatrix}x^C_t\\ x^R_t\end{pmatrix}.
\]

\medskip
Thus, the long-run stable distribution of the population is given by an
\textcolor{SeaGreen}{eigenvector} of $A$, and $\lambda$ is the associated
\textcolor{SeaGreen}{eigenvalue} describing the growth rate.

\end{frame}
\begin{frame}{Eigenvalues and eigenvectors: definitions}
 
For $A\in\R^{n\times n}$, $\lambda\in\R$ is an \textcolor{SeaGreen}{eigenvalue} if there exists ${\bf x}\ne{\bf 0}$ with
\[
A{\bf x}=\lambda{\bf x}\quad\Longleftrightarrow\quad (A-\lambda   I_n){\bf x}={\bf 0}.
\]
Then ${\bf x}$ is an \textcolor{SeaGreen}{eigenvector} for $\lambda$.
\vskip 6pt
\textcolor{SeaGreen}{Characterization:}
\[
\lambda\ \text{eigenvalue}\ \Longleftrightarrow\ \rank(A-\lambda  I_n)<n\ \Longleftrightarrow\ \det(A-\lambda  I_n)=0.
\]
The \textcolor{SeaGreen}{characteristic polynomial} $P(\lambda)=\det(A-\lambda  I_n)$ has degree $n$; the real roots (at most $n$) are the eigenvalues.
\vskip 6pt
\textcolor{SeaGreen}{Procedure:}
\begin{enumerate}
\item Solve $\det(A-\lambda  I_n)=0$ to get eigenvalues.
\item For each $\lambda$, solve $(A-\lambda  I_n){\bf x}={\bf 0}$ for eigenvectors.
\end{enumerate}
 
\end{frame}

\begin{frame}{Cohort-mix example (continued)}
 
With
\(
A=\begin{pmatrix}0.8&0.4\\ 0.3&0.9\end{pmatrix}
\),
\[
(0.8-\lambda)(0.9-\lambda)-0.12=0
\ \Longleftrightarrow\
\lambda^2-1.7\lambda+0.6=0.
\]
The solutions are $\lambda_1=1.2$ and $\lambda_2=0.5$.
\vskip 4pt
For $\lambda_1=1.2$, eigenvectors are multiples of $(1,1)$. For $\lambda_2=0.5$, eigenvectors are multiples of $(1,-\tfrac{3}{4})$.
\vskip 4pt
{\tiny For a population-growth interpretation we keep the dominant eigenpair: equal cohort proportions grow by $20\%$ per step.}
 
\end{frame}

\begin{frame}{Basic spectral facts}
\begin{block}{}
If $A\in\R^{n\times n}$,
\begin{enumerate}
\item Eigenvectors associated with distinct eigenvalues are \textcolor{SeaGreen}{linearly independent}.

\item If $A$ is \textcolor{SeaGreen}{symmetric}, then all eigenvalues are real and eigenvectors for distinct eigenvalues are \textcolor{SeaGreen}{orthogonal} (Spectral Theorem).
\end{enumerate}
\end{block}

\bigskip
\textcolor{SeaGreen}{Example:}
\[
A=\begin{pmatrix}1&2\\ 2&1\end{pmatrix}.
\]
Then $P(\lambda)=\lambda^2-2\lambda-3$, so $\lambda_1=3$, $\lambda_2=-1$ with eigenvectors along $(1,1)$ and $(-1,1)$, respectively.

\end{frame}



\begin{frame}{Repeated eigenvalues}
Consider
\[
A=\begin{pmatrix}
0&-1&1\\
-7&0&5\\
-5&-2&5
\end{pmatrix}.
\]
The characteristic polynomial is
\[
p_A(\lambda)=\lambda^3-5\lambda^2+8\lambda-4=(\lambda-2)^2(\lambda-1).
\]
Hence the eigenvalues are $\lambda=2$ (algebraic multiplicity $2$) and $\lambda=1$ (algebraic multiplicity $1$).
\vskip6pt
\textcolor{SeaGreen}{Eigenspaces:}
\[
E_{2}=\ker(A-2I)=\operatorname{span}\!\left\{(1,-1,1)\right\}, 
\qquad
E_{1}=\ker(A-I)=\operatorname{span}\!\left\{(2,1,3)\right\}.
\]
Since $\dim E_{2}=1<2$, $A$ is \textcolor{SeaGreen}{defective} (not diagonalizable). 
\end{frame}



\begin{frame}{Application: Principal Component Analysis (PCA)} 
\begin{small}
We observe $n$ data points ${\bf x}_1,\ldots,{\bf x}_n \in \R^d$.  
\medskip

\textcolor{SeaGreen}{Step 1 (Centering):} Arrange the data in a matrix $\bs X\in \R^{n\times d}$ with rows $\bs x_1,\ldots,\bs x_n$. Define the centering matrix $H=I_n-\tfrac{1}{n}{\bf 1}{\bf 1}^\top $ and 
then the \alert{centered data} is $H\bs X$.
\bigskip

\textcolor{SeaGreen}{Step 2 (Sample covariance):} 
\[
S=\tfrac{1}{n}(H\bs X)^\top  (H\bs X).
\]
\medskip

\textcolor{SeaGreen}{Step 3 (Spectral decomposition):}  
\[
S = U \Lambda U^\top\quad\mbox{ and define } \bs Y=H\bs XU \quad \mbox{(rotate the centered data)}.
\]
The sample covariance of $\bs Y$ is $\tfrac1n \bs Y^\top \bs Y=U^\top  S U=\Lambda$ (diagonal!).\\[3mm]

The columns of $U$ are \textcolor{SeaGreen}{principal directions} and diagonal entries of $\Lambda$ are the \textcolor{SeaGreen}{explained variances}.
\medskip

Sorting eigenvalues in decreasing order ranks components by variance captured.  
\end{small}
\end{frame}


\begin{frame}{PCA: why it matters}

\begin{minipage}{7cm}
\includegraphics[width=0.95\textwidth]{img/genomemap}	
\end{minipage}\begin{minipage}{8cm}
\textcolor{SeaGreen}{Dimensionality reduction:} compress high-dimensional data into a few directions that preserve most variability.\\[3mm]
\textcolor{SeaGreen}{Noise filtering:} small eigenvalues $\to$ directions with little signal.\\[3mm]
	\textcolor{SeaGreen}{Visualization:} projecting onto first 2 PCs often reveals hidden structure.\\[3mm]
	\textcolor{SeaGreen}{Nice example:} genetic variation in Europeans â plotting individuals on the first two PCs of SNP data \textit{recovers the geographic map of Europe}!  
\end{minipage}
\end{frame}


\begin{frame}{Numerical toy example}
\begin{small}
Covariance matrix:
\[
S=
\begin{pmatrix}
72.9&87.5&6.875\\
87.5&108.3&8.75\\
6.875&8.75&0.73
\end{pmatrix}.
\]

Spectral decomposition:
\[
S = U \Lambda U^\top, \quad
\Lambda=\mathrm{diag}(180,\ 1.38,\ 0.005).
\]

$\Rightarrow$ The first component explains $>99\%$ of the variance; the last can be safely ignored.  
\end{small}
\end{frame}


\begin{frame}{PCA \& Unique football styles}
``\textit{Mille viae ducunt Barcinonem}''\\ 
(A thousand roads lead to Barcelona)\\[3mm]

\begin{minipage}{7cm}
	\includegraphics[width=\textwidth]{img/tikitaka}
\end{minipage}
\begin{minipage}{8cm}
	``Searching for a Unique Style in Soccer'' (Gyarmati, Kwak \& Rodriguez, 2014), studies sequential pass patterns in a team's passing.\\[3mm]
They defined ``flow motifs'' as statically significant pass subsequences (e.g., $A\to B\to C\to A$).\\[3mm]
Applied motif analysis to compare styles across teams and leagues. The data are projected to the first two principal components.
\end{minipage}
\end{frame}


\begin{frame}{Quadratic forms}
A \textcolor{SeaGreen}{quadratic form} is $Q({\bf x})={\bf x}^\top  A{\bf x}=\sum_{i,j=1}^n a_{ij}x_ix_j$ with $A\in\R^{n\times n}$ symmetric.
\vskip 6pt
\textcolor{SeaGreen}{Remark:} Since the coefficient of $x_ix_j$ is $a_{ij}+a_{ji}$, every quadratic form has a unique symmetric representative (replace $A$ by $\tfrac{1}{2}(A+A^\top )$).
\vskip 6pt
\textcolor{SeaGreen}{Example:}
\(
\begin{pmatrix}1&2\\ 3&4\end{pmatrix}
\)
and
\(
\begin{pmatrix}1&5/2\\ 5/2&4\end{pmatrix}
\)
define the same $Q(x,y)=x^2+5xy+4y^2$.
 
\end{frame}

\begin{frame}{Signs of quadratic forms}
 
For symmetric $A$:
\begin{enumerate}
\item \textcolor{SeaGreen}{Positive definite}: ${\bf x}^\top  A{\bf x}>0$ for all ${\bf x}\ne{\bf 0}$.
\item \textcolor{SeaGreen}{Negative definite}: ${\bf x}^\top  A{\bf x}<0$ for all ${\bf x}\ne{\bf 0}$.
\item \textcolor{SeaGreen}{Positive semidefinite}: ${\bf x}^\top  A{\bf x}\ge 0$ for all ${\bf x}$.
\item \textcolor{SeaGreen}{Negative semidefinite}: ${\bf x}^\top  A{\bf x}\le 0$ for all ${\bf x}$.
\item \textcolor{SeaGreen}{Indefinite}: takes both positive and negative values.
\end{enumerate}
\textcolor{SeaGreen}{Example:}
\[
{\bf x}^\top 
\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}
{\bf x}=(x-y)^2\ge 0
\]
so the matrix is positive semidefinite.
 
\end{frame}

\begin{frame}{Eigenvalue and minor tests}
 
\textcolor{SeaGreen}{Spectral test (symmetric $A$):}
\begin{itemize}
\item PD $\Longleftrightarrow$ all eigenvalues $>0$;\quad ND $\Longleftrightarrow$ all $<0$;
\item PSD $\Longleftrightarrow$ all $\ge 0$;\quad NSD $\Longleftrightarrow$ all $\le 0$;
\item Indefinite $\Longleftrightarrow$ eigenvalues of different signs.
\end{itemize}
\textcolor{SeaGreen}{Sylvesterâs criterion (leading principal minors $D_k$):}
\begin{itemize}
\item PD $\Longleftrightarrow$ $D_k>0$ for $k=1,\ldots,n$;
\item ND $\Longleftrightarrow$ $(-1)^kD_k>0$ for $k=1,\ldots,n$.
\end{itemize}
\textcolor{SeaGreen}{Example:}
\(
A=\begin{pmatrix}-3&2&0\\ 2&-3&0\\ 0&0&-5\end{pmatrix}
\)
has $D_1=-3$, $D_2=5$, $D_3=-25$ $\Rightarrow$ negative definite.
 
\end{frame}

\begin{frame}{Examples}
 
\begin{enumerate}
\item
$
A=\begin{pmatrix}1&4&6\\ 4&2&1\\ 6&1&6\end{pmatrix}
$:
$D_1=1$, $D_2=-14$, $D_3=-109$ $\Rightarrow$ \textcolor{SeaGreen}{indefinite}.
\item
$
A=\begin{pmatrix}3&-1&0\\ -1&2&-1\\ 0&-1&3\end{pmatrix}
$:
$D_1=3$, $D_2=5$, $D_3=12$ $\Rightarrow$ \textcolor{SeaGreen}{positive definite}.
\item The symmetric matrix of $Q(x,y,z)=xy+yz$ is indefinite since $D_1=D_3=0$ and $D_2=-\tfrac14$.
\end{enumerate}
 
\end{frame}


\end{document}